package tddc17;

import aima.core.environment.liuvacuum.*;
import aima.core.agent.Action;
import aima.core.agent.AgentProgram;
import aima.core.agent.Percept;
import aima.core.agent.impl.*;

import java.util.AbstractMap;
import java.util.AbstractMap.SimpleEntry;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import java.util.Random;
import java.util.Stack;

class MyAgentState {
	public int[][] world = new int[30][30];
	public int initialized = 0;
	final int UNKNOWN = 0;
	final int WALL = 1;
	final int CLEAR = 2;
	final int DIRT = 3;
	final int HOME = 4;
	final int ACTION_NONE = 0;
	final int ACTION_MOVE_FORWARD = 1;
	final int ACTION_TURN_RIGHT = 2;
	final int ACTION_TURN_LEFT = 3;
	final int ACTION_SUCK = 4;

	public int agent_x_position = 1;
	public int agent_y_position = 1;
	public int agent_last_action = ACTION_NONE;

	public static final int NORTH = 0;
	public static final int EAST = 1;
	public static final int SOUTH = 2;
	public static final int WEST = 3;
	public int agent_direction = EAST;

	MyAgentState() {
		for (int i = 0; i < world.length; i++)
			for (int j = 0; j < world[i].length; j++)
				world[i][j] = UNKNOWN;
		world[1][1] = HOME;
		agent_last_action = ACTION_NONE;
	}

	// Based on the last action and the received percept updates the x & y agent
	// position
	public void updatePosition(DynamicPercept p) {
		Boolean bump = (Boolean) p.getAttribute("bump");

		if (agent_last_action == ACTION_MOVE_FORWARD && !bump) {
			switch (agent_direction) {
			case MyAgentState.NORTH:
				agent_y_position--;
				break;
			case MyAgentState.EAST:
				agent_x_position++;
				break;
			case MyAgentState.SOUTH:
				agent_y_position++;
				break;
			case MyAgentState.WEST:
				agent_x_position--;
				break;
			}
		}

	}

	public void updateWorld(int x_position, int y_position, int info) {
		world[x_position][y_position] = info;
	}

	public void printWorldDebug() {
		for (int i = 0; i < world.length; i++) {
			for (int j = 0; j < world[i].length; j++) {
				if (world[j][i] == UNKNOWN)
					System.out.print(" ? ");
				if (world[j][i] == WALL)
					System.out.print(" # ");
				if (world[j][i] == CLEAR)
					System.out.print(" . ");
				if (world[j][i] == DIRT)
					System.out.print(" D ");
				if (world[j][i] == HOME)
					System.out.print(" H ");
			}
			System.out.println("");
		}
	}
}

class MyAgentProgram implements AgentProgram {

	private int initnialRandomActions = 10;
	private Random random_generator = new Random();

	// Here you can define your variables!
	// public int iterationCounter = 10;
	public MyAgentState state = new MyAgentState();
    	private boolean start_up = true;
	private Stack<int[]> nodesToExplore = new Stack<>();
	private List<Entry<int[], List<int[]>>> node_parents_list = new ArrayList<>();

	// moves the Agent to a random start position
	// uses percepts to update the Agent position - only the position, other
	// percepts are ignored
	// returns a random action
	private Action moveToRandomStartPosition(DynamicPercept percept) {
		int action = random_generator.nextInt(6);
		initnialRandomActions--;
		state.updatePosition(percept);
		if (action == 0) {
			state.agent_direction = ((state.agent_direction - 1) % 4);
			if (state.agent_direction < 0)
				state.agent_direction += 4;
			state.agent_last_action = state.ACTION_TURN_LEFT;
			return LIUVacuumEnvironment.ACTION_TURN_LEFT;
		} else if (action == 1) {
			state.agent_direction = ((state.agent_direction + 1) % 4);
			state.agent_last_action = state.ACTION_TURN_RIGHT;
			return LIUVacuumEnvironment.ACTION_TURN_RIGHT;
		}
		state.agent_last_action = state.ACTION_MOVE_FORWARD;
		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
	}

	@Override
	public Action execute(Percept percept) {

		// DO NOT REMOVE this if condition!!!
		if (initnialRandomActions > 0) {
			return moveToRandomStartPosition((DynamicPercept) percept);
		} else if (initnialRandomActions == 0) {
			// process percept for the last step of the initial random actions
			initnialRandomActions--;
			state.updatePosition((DynamicPercept) percept);
			System.out
					.println("Processing percepts after the last execution of moveToRandomStartPosition()");
			state.agent_last_action = state.ACTION_SUCK;
			return LIUVacuumEnvironment.ACTION_SUCK;
		}

		// This example agent program will update the internal agent state while
		// only moving forward.
		// START HERE - code below should be modified!

		System.out.println("x=" + state.agent_x_position);
		System.out.println("y=" + state.agent_y_position);
		System.out.println("dir=" + state.agent_direction);

		/*
		 * iterationCounter--;
		 * 
		 * if (iterationCounter==0) return NoOpAction.NO_OP;
		 */

		DynamicPercept p = (DynamicPercept) percept;
		Boolean bump = (Boolean) p.getAttribute("bump");
		Boolean dirt = (Boolean) p.getAttribute("dirt");
		Boolean home = (Boolean) p.getAttribute("home");
		System.out.println("percept: " + p);

		// State update based on the percept value and the last action
		state.updatePosition((DynamicPercept) percept);
		if (bump) {
			switch (state.agent_direction) {
			case MyAgentState.NORTH:
				state.updateWorld(state.agent_x_position,
						state.agent_y_position - 1, state.WALL);
				break;
			case MyAgentState.EAST:
				state.updateWorld(state.agent_x_position + 1,
						state.agent_y_position, state.WALL);
				break;
			case MyAgentState.SOUTH:
				state.updateWorld(state.agent_x_position,
						state.agent_y_position + 1, state.WALL);
				break;
			case MyAgentState.WEST:
				state.updateWorld(state.agent_x_position - 1,
						state.agent_y_position, state.WALL);
				break;
			}
		}
		if (dirt)
			state.updateWorld(state.agent_x_position, state.agent_y_position,
					state.DIRT);
		else
			state.updateWorld(state.agent_x_position, state.agent_y_position,
					state.CLEAR);

		state.printWorldDebug();

		// main rotation loop starts here!
		// Next action selection based on the percept value
		if (bump) {
			nodesToExplore.pop();
		}
		
		if (dirt) {
			System.out.println("DIRT -> choosing SUCK action!");
			state.agent_last_action = state.ACTION_SUCK;
			return LIUVacuumEnvironment.ACTION_SUCK;
		}
	    	if (start_up) {
		    // add startNode to queue
		    int[] startNode = { state.agent_x_position, state.agent_y_position };
		    nodesToExplore.add(startNode);
		    List<int[]> start_node_parents = new ArrayList<>();
		    node_parents_list.add(new AbstractMap.SimpleEntry<>(startNode, start_node_parents));
		    start_up = false;
		}
		System.out.println("before queue check");
		if (!nodesToExplore.isEmpty()) {
			int[] targetNode = nodesToExplore.peek();
			if (!Arrays.equals(targetNode, new int[]{state.agent_x_position, state.agent_y_position})) {
			    return goToTargetNode(targetNode, node_parents_list);
			}
			nodesToExplore.pop();
			System.out.println("after target node removal");
			if (!nodesToExplore.contains(new int[]{state.agent_x_position, state.agent_y_position - 1}) &&
			    state.world[state.agent_x_position][state.agent_y_position - 1] == state.UNKNOWN ||
			    state.world[state.agent_x_position][state.agent_y_position - 1] == state.HOME) { // Add northnode
				addUnexploredNeighbourToNodesToExplore(targetNode, state.agent_x_position, state.agent_y_position - 1);
			}
			if (!nodesToExplore.contains(new int[]{state.agent_x_position + 1, state.agent_y_position}) &&
						    state.world[state.agent_x_position + 1][state.agent_y_position] == state.UNKNOWN ||
			    state.world[state.agent_x_position + 1][state.agent_y_position] == state.HOME) { // Add eastnode
				addUnexploredNeighbourToNodesToExplore(targetNode, state.agent_x_position + 1, state.agent_y_position);
			}
			if (!nodesToExplore.contains(new int[]{state.agent_x_position, state.agent_y_position + 1}) &&
						    state.world[state.agent_x_position][state.agent_y_position + 1] == state.UNKNOWN ||
			    state.world[state.agent_x_position][state.agent_y_position + 1] == state.HOME) { // Add southnode
				addUnexploredNeighbourToNodesToExplore(targetNode, state.agent_x_position, state.agent_y_position + 1);
			}
			if (!nodesToExplore.contains(new int[]{state.agent_x_position - 1, state.agent_y_position}) &&
						    state.world[state.agent_x_position - 1][state.agent_y_position] == state.UNKNOWN ||
			    state.world[state.agent_x_position - 1][state.agent_y_position] == state.HOME) { // Add westnode
				addUnexploredNeighbourToNodesToExplore(targetNode, state.agent_x_position - 1, state.agent_y_position);
			}
			System.out.println("after neighbor to queue add");
			return goToTargetNode(nodesToExplore.peek(), node_parents_list);
		} else {
			// goHome();
			return NoOpAction.NO_OP;
		}
	}
	
	private void addUnexploredNeighbourToNodesToExplore(int[] parentNode, int x, int y) {
		System.out.println("added node x:" + x + ", y: " + y);
		int[] nodeToVisit = {x, y};
		nodesToExplore.add(nodeToVisit);			
		List<int[]> parents_nodes = new ArrayList<>(getParentNodes(parentNode));
		parents_nodes.add(parentNode);
		node_parents_list.add(new AbstractMap.SimpleEntry<>(nodeToVisit, parents_nodes));
	}

	private Action turnRight() {
		state.agent_direction = ((state.agent_direction + 1) % 4);
		state.agent_last_action = state.ACTION_TURN_RIGHT;
		return LIUVacuumEnvironment.ACTION_TURN_RIGHT;
	}

	private Action turnLeft() {
		state.agent_direction = ((state.agent_direction - 1) % 4);
		if (state.agent_direction < 0)
			state.agent_direction += 4;
		state.agent_last_action = state.ACTION_TURN_LEFT;
		return LIUVacuumEnvironment.ACTION_TURN_LEFT;
	}

	private Action walkForward() {
		state.agent_last_action = state.ACTION_MOVE_FORWARD;
		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
	}
	
	private Action goToTargetNode(int[] targetNode, List<Entry<int[], List<int[]>>> node_parents_list) {
	    	System.out.println("gototargetnode, target: " + targetNode[0] + ", " + targetNode[1]);
		List<int[]> target_node_parents = getParentNodes(targetNode);
	    	int[] currentNode = {state.agent_x_position, state.agent_y_position};
		List<int[]> current_node_parents = getParentNodes(currentNode);
		int[] nextNode;
	    System.out.println("target node parents size: " + target_node_parents.size());
	    System.out.println(target_node_parents.get(0)[0] + ", " + target_node_parents.get(0)[1]);
	    System.out.println("current node parents size: " + current_node_parents.size());
	    System.out.println("current node: " + currentNode[0] + ", " + currentNode[1]);
	    	boolean subSet = false;
	    	if (current_node_parents.size() < target_node_parents.size()) {
		    // kolla om äkta delmängd
		    subSet = true;
		    List<int[]> current_nodes = new ArrayList<>(current_node_parents);
		    current_nodes.add(currentNode);
		    for (int i = 0; i<current_nodes.size(); i++) {
			if (!Arrays.equals(current_nodes.get(i), target_node_parents.get(i))) {
			    subSet = false;
			    break;
			}
		    }
		}
	    	System.out.println("subset check done");
	    System.out.println(subSet);
	    	if (subSet && !Arrays.equals(currentNode, target_node_parents.get(current_node_parents.size()))) {
		    	// follow the targets trail of nodes
		    	nextNode = target_node_parents.get(current_node_parents.size());
		    System.out.println("next 1");
		} else if (Arrays.equals(target_node_parents.get(target_node_parents.size()-1), currentNode))  {
		    	// almost at target
		    	nextNode = targetNode;
		    System.out.println("next 2");
		} else if (subSet && current_node_parents.size() == 0) {
		    	// at start-position
				    System.out.println("next 3");
				    nextNode = target_node_parents.get(1);
		} else if (subSet) {
		    nextNode = target_node_parents.get(current_node_parents.size()+1);
		    System.out.println("next 4");
		}
		else { // going back to start-position
		    	nextNode = current_node_parents.get(current_node_parents.size()-1);
		    System.out.println("next 5");
		}
	    System.out.println("next node: " + nextNode[0] + ", " + nextNode[1]);
		int[] nodeInFront = null;
		int deltaX = 0;
		int deltaY = 0;
		switch (state.agent_direction) {
		    case MyAgentState.NORTH:
			    nodeInFront = new int[]{state.agent_x_position, state.agent_y_position-1};
			    deltaY = -1;
			    break;
		    case MyAgentState.EAST:
			    nodeInFront = new int[]{state.agent_x_position+1, state.agent_y_position};
			    deltaX = 1;
			    break;
		    case MyAgentState.SOUTH:
			    nodeInFront = new int[]{state.agent_x_position, state.agent_y_position+1};
			    deltaY = 1;
			    break;
		    case MyAgentState.WEST:
			    nodeInFront = new int[]{state.agent_x_position-1, state.agent_y_position};
			    deltaX = -1;
			    break;
		}
		
		if (Arrays.equals(nodeInFront, nextNode)) {
			return walkForward();
		}else {
			nodeInFront = new int[]{state.agent_x_position + deltaY, state.agent_y_position - deltaX};
			return turnRight();
		}
	}
	
	private List<int[]> getParentNodes(int[] node) {
		List<int[]> result = null;
		for (int i = 0; i< node_parents_list.size(); i++) {
		    if (Arrays.equals(node_parents_list.get(i).getKey(), node)) {
			    result = node_parents_list.get(i).getValue();
				break;
		    }
		}
		return result;
	} 
}

public class MyVacuumAgent extends AbstractAgent {
	public MyVacuumAgent() {
		super(new MyAgentProgram());
	}
}
